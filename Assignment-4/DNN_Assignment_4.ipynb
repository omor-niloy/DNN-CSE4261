{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFtFifmjZS+5URc8hdU4Lo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omor-niloy/DNN-CSE4261/blob/main/Assignment-4/DNN_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9VMk1FJke-41"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess MNIST\n",
        "(x_train_full, y_train_full), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train_full = x_train_full.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train_full = np.expand_dims(x_train_full, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_full = to_categorical(y_train_full, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# 0.1 validation split\n",
        "val_size = int(0.1 * len(x_train_full))  # 6000\n",
        "x_val, y_val = x_train_full[:val_size], y_train_full[:val_size]\n",
        "x_train, y_train = x_train_full[val_size:], y_train_full[val_size:]\n",
        "\n",
        "# tf.data Datasets\n",
        "batch_size = 64\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
      ],
      "metadata": {
        "id": "BBdgCxnJgbm1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    return models.Sequential([\n",
        "        layers.Flatten(input_shape=(28, 28, 1)),  # Flatten image\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10)  # logits (no softmax)\n",
        "    ])"
      ],
      "metadata": {
        "id": "LDgvIBDsgvzy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbTcDT46iKdh",
        "outputId": "afc1f1d1-5a5e-4ace-a244-ceeda602d3ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in dataset:\n",
        "        logits = model(x, training=False)\n",
        "        preds = tf.argmax(logits, axis=1)\n",
        "        labels = tf.argmax(y, axis=1)\n",
        "        correct += tf.reduce_sum(tf.cast(preds == labels, tf.int32))\n",
        "        total += y.shape[0]\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "YvoUfbMjiMSF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "print(\"=== Manual Training with GradientTape ===\")\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for x_batch, y_batch in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_losses.append(loss.numpy())\n",
        "\n",
        "        # Accuracy calculation\n",
        "        preds = tf.argmax(logits, axis=1)\n",
        "        true_labels = tf.argmax(y_batch, axis=1)\n",
        "        train_correct += tf.reduce_sum(tf.cast(preds == true_labels, tf.int32))\n",
        "        train_total += y_batch.shape[0]\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "    train_accuracy = train_correct / train_total\n",
        "\n",
        "    # Validation loop\n",
        "    val_losses = []\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    for x_val_batch, y_val_batch in val_ds:\n",
        "        val_logits = model(x_val_batch, training=False)\n",
        "        val_loss = loss_fn(y_val_batch, val_logits)\n",
        "        val_losses.append(val_loss.numpy())\n",
        "\n",
        "        preds = tf.argmax(val_logits, axis=1)\n",
        "        true_labels = tf.argmax(y_val_batch, axis=1)\n",
        "        val_correct += tf.reduce_sum(tf.cast(preds == true_labels, tf.int32))\n",
        "        val_total += y_val_batch.shape[0]\n",
        "\n",
        "    val_loss = np.mean(val_losses)\n",
        "    val_accuracy = val_correct / val_total\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54AwUNtJiQm4",
        "outputId": "9bdd5c7c-ca00-402c-d002-378d7f26eff6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Manual Training with CategoricalCrossentropy ===\n",
            "\n",
            "Epoch 1/5\n",
            "Train Loss: 0.2547, Train Acc: 0.9261, Val Loss: 0.1148, Val Acc: 0.9652\n",
            "\n",
            "Epoch 2/5\n",
            "Train Loss: 0.0990, Train Acc: 0.9701, Val Loss: 0.0981, Val Acc: 0.9682\n",
            "\n",
            "Epoch 3/5\n",
            "Train Loss: 0.0656, Train Acc: 0.9789, Val Loss: 0.0867, Val Acc: 0.9743\n",
            "\n",
            "Epoch 4/5\n",
            "Train Loss: 0.0495, Train Acc: 0.9843, Val Loss: 0.0783, Val Acc: 0.9770\n",
            "\n",
            "Epoch 5/5\n",
            "Train Loss: 0.0378, Train Acc: 0.9876, Val Loss: 0.0813, Val Acc: 0.9790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = evaluate(model, test_ds)\n",
        "print(f\"\\nManual Test Accuracy: {acc.numpy() * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRRn2yC-iV7Q",
        "outputId": "a69f8abc-4669-4099-f78a-cb7a2f73fb40"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Manual Test Accuracy: 97.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Training with model.fit() ===\n",
        "model = create_model()\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "JaePXdnviaaJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training with model.fit() ===\")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        "print(\"\\n=== Evaluation with model.evaluate() ===\")\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, batch_size=64)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wyje6fKieb4",
        "outputId": "9acbfd6f-0895-49fc-dc78-161f7f636fe9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training with model.fit() ===\n",
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8589 - loss: 0.4672 - val_accuracy: 0.9608 - val_loss: 0.1248\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9675 - loss: 0.1041 - val_accuracy: 0.9710 - val_loss: 0.0956\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0655 - val_accuracy: 0.9730 - val_loss: 0.0888\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9845 - loss: 0.0483 - val_accuracy: 0.9663 - val_loss: 0.1174\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9887 - loss: 0.0337 - val_accuracy: 0.9742 - val_loss: 0.0972\n",
            "\n",
            "=== Evaluation with model.evaluate() ===\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0957\n",
            "Test Accuracy: 97.51%\n"
          ]
        }
      ]
    }
  ]
}